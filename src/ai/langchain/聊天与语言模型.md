
> Langchain 的核心功能之一，就是通过提供统一的接口，使得开发者能够快速接入各种大型语言模型（LLM）。类似于 Langchain，Langchain4j 也实现了对不同大模型的统一访问，简化了 API 的调用流程，同时规范了输入输出参数的格式，帮助开发者更加高效地集成 LLM。在接下来的系列课程中，我们将详细介绍如何通过 Langchain4j 接入 LLM，并深入探讨其输入输出参数的结构和应用场景。

# ChatLanguageModel
`ChatLanguageModel` 是与大模型交互的基础类，它支持接收 `String`（内部会转换为 `ChatMessage` 对象）或一个或多个 `ChatMessage` 对象作为输入，返回 `AiMessage` 对象作为输出。

在与大型语言模型（LLM）的对话中，`ChatMessage` 通常代表纯文本消息。但对于一些高级的 LLM，特别是多模态模型，不仅支持纯文本，还能处理文本与图像的混合输入。像OpenAI的`gpt-3.5-turbo`、多模态版本的`gpt-4` 以及 Google 的 `gemini-pro` 等模型，都具备处理文本和图像混合输入的能力。

下面来看一下 `ChatLanguageModel` 的主要API：

```java
/**
 * 大模型对话接口
 */
public interface ChatLanguageModel {

    default String generate(String userMessage) {
        return  generate(UserMessage.from(userMessage)).content().text();
    }

    default Response<AiMessage> generate(ChatMessage... messages) {
        return generate(asList(messages));
    }

    Response<AiMessage> generate(List<ChatMessage> messages);
}
```
最简单的 `generate` 方法可以直接接受 `String` 作为输入并返回 `String` 作为输出。这是一种便捷方式，旨在简化使用流程，避免手动将输入包装成 `ChatMessage` 对象。

此外还有一些额外的API：
```java
default Response<AiMessage> generate(List<ChatMessage> messages, List<ToolSpecification> toolSpecifications) {
        throw new IllegalArgumentException("Tools are currently not supported by this model");
    }


    default Response<AiMessage> generate(List<ChatMessage> messages, ToolSpecification toolSpecification) {
        throw new IllegalArgumentException("Tools are currently not supported by this model");
    }

    @Experimental
    default ChatResponse chat(ChatRequest request) {
        throw new UnsupportedOperationException();
    }

    @Experimental
    default Set<Capability> supportedCapabilities() {
        return emptySet();
    }
```
这些API是相对高级的功能，但是始终是以聊天对话为基础。在后续的课程会讲到相关的使用方法。

# ChatMessage
在了解 `ChatMessage` 之前，首先需要掌握大模型对话中的角色概念。

在大模型驱动的对话中，API 参数中常见的一个关键元素是“角色”（Role），它决定了每条消息在对话中的身份和功能，至关重要。通常，角色分为三类：`system`、`user` 和 `assistant`，每个角色都有其特定的用途和行为模式。

- **System (系统)**
   - **作用**: 定义对话的整体行为、特定的行为准则、基调、风格、目标。
   - **用途**: 通过系统消息，开发者可以指定模型在整个对话过程中应如何响应用户。比如，可以指示模型扮演一个帮助用户进行编程的助手，或者是一个帮助解决客户服务问题的代表。
   - **示例**: 
     ```json
     {
     "role": "system", 
     "content": "You are a helpful assistant that provides concise and accurate information."
     }
     ```

- **User (用户)**
   - **作用**: 提供用户的输入或指令。
   - **用途**:用户消息代表实际用户的输入，是模型响应的主要依据。它们包含了用户的问题、请求或任何其他输入内容。 用户角色是对话的发起者，模型根据用户消息来生成相应的回复。
   - **示例**:
     ```json
     {"role": "user", 
     "content": "Can you help me write a Python function to sort a list?"
     }
     ```

- **Assistant (助手)**
   - **作用**: 模型的回复。
   - **用途**:助手消息是由AI模型生成的响应，基于用户的输入以及系统设定的行为规则。这是模型进行信息传达和互动的主要方式。助手角色负责执行由用户提出的问题或任务，并提供相应的回复或建议。
   - **示例**:
     ```json
     {
     "role": "assistant", 
     "content": "Sure! Here's a Python function that sorts a list."
     }
     ```
`ChatMessage` 是一个接口。为处理不同类型的消息，Langchain4j 提供了多个 `ChatMessage` 的实现对象。这些实现对象能够处理各种消息类型，从而确保与大型语言模型（LLM）的交互更加灵活和高效。

通过这些实现，开发者可以轻松地创建和管理不同类型的聊天消息，以满足具体的应用需求和场景:

- **SystemMessage**：作为开发人员，您通常需要定义此消息的内容，包括有关 LLM 在对话中的角色、行为方式和回答风格等说明。`SystemMessage` 帮助设置对话的上下文和规则，LLM 会更关注这些消息，因此请小心处理，避免让最终用户自由定义或注入内容。`SystemMessage` 通常位于对话的开头，用于设定对话的框架和规则。

- **UserMessage**：来自用户的消息。用户可以是您的应用程序的最终用户（人类）或应用程序本身。根据 LLM 的支持，`UserMessage` 可以仅包含文本（`String`），也可以包含文本和/或图像（`Image`），根据实际需求进行设置。

- **AiMessage**：由 AI 生成的消息，通常用于响应 `UserMessage`。`generate` 方法返回一个 `AiMessage`，可能包装在一个响应对象中。`AiMessage` 可以包含文本响应（`String`）或工具执行请求（`ToolExecutionRequest`）。我们将在稍后详细探讨这些工具的使用。


此外，Langchain4j 还定义了一种消息类型：

- **ToolExecutionResultMessage**：在大模型的函数调用功能中，函数的实际实现并不是由大模型直接提供的，而是由开发者在服务器或应用程序的后端定义和实现。大模型的作用是根据用户的输入判断何时需要调用某个函数，并生成所需的参数。具体的函数逻辑和执行则由开发者负责，大模型仅负责生成调用方法的参数，并决定调用哪个函数。

在后续的文章中，将详细介绍`ToolExecutionResultMessage`。


# Response
在最简单的情况下，我们可以直接调用 `ChatLanguageModel` 的 `generate` 函数，该函数返回 `String`。然而，在大多数应用场景中，通常调用返回 `Response<AiMessage>` 的函数。`Response` 是一个包装器，不仅包含大模型返回的核心内容，还包括相关的元数据:
```java
/**
 * 表示来自各种模型的响应类型，如语言模型、聊天模型、嵌入模型和内容审核模型。
 * 该类封装了生成的内容、令牌使用统计信息以及结束原因。
 *
 * @param <T> 模型生成的内容类型。
 */
public class Response<T> {

    private final T content;
    private final TokenUsage tokenUsage;
    private final FinishReason finishReason;

    // 构造方法、getter 方法省略
}
```
## TokenUsage
`Response` 包含一个 `TokenUsage` 对象，用于提供令牌使用情况的详细统计信息。它包括：	
- 输入令牌数量：由用户提供的 `ChatMessages` 所使用的令牌数。
- 输出令牌数量：模型生成的 `AiMessage` 中包含的令牌数。	
- 总令牌数：输入和输出令牌的总和。

```java
public class TokenUsage {
    // 成员变量
    private final Integer inputTokenCount;   // 输入文本中的令牌数量
    private final Integer outputTokenCount;  // 输出文本中的令牌数量
    private final Integer totalTokenCount;   // 输入和输出的令牌总数

    // 构造方法、getter 方法省略
}
```
这些数据对于计算 LLM 调用的成本至关重要，帮助开发者评估和控制资源消耗。


## FinishReason
`Response` 还包含 `FinishReason`，这是一个枚举类型，用于表示模型调用结束的原因。它定义了几种可能的情况，帮助解释为什么模型生成过程被终止:

- **STOP**  
   - **含义**：模型调用因为模型本身认为请求已经完成而结束。  
   - **典型场景**：模型在生成的响应中完成了用户请求的所有信息后，自动停止生成进一步的内容。

- **LENGTH**  
   - **含义**：调用因为达到最大令牌长度（token length）限制而结束。  
   - **典型场景**：生成的响应内容过长，超出了预设的令牌限制，导致模型被迫停止继续生成。

- **TOOL_EXECUTION**  
   - **含义**：调用结束时，模型提示需要执行某个工具或函数。  
   - **典型场景**：模型判断当前需要外部工具（如函数调用、API调用等）来获取额外信息或执行操作。

- **CONTENT_FILTER**  
   - **含义**：调用因为内容过滤机制而结束。  
   - **典型场景**：模型检测到生成的内容可能包含不适当或有害信息，因此终止了响应。这通常涉及到内容审核模型的使用。

- **OTHER**  
   - **含义**：调用因为其他原因而结束。  
   - **典型场景**：任何不属于上述原因的调用结束情况都归类为此。

# 完整的示例
现在我们已经了解了所有类型的`ChatMessage` 以及对应的 `Response`，接下来我们将展示如何在对话中结合使用它们。由于 `ChatMessage` 的用法较为简单，这里将不单独展示，而是结合 `Response` 的使用进行讲解。
```java
public class ChatLanguageDemo1 {


    public static void main(String[] args) {
        ChatLanguageModel chatLanguageModel = getChatLanguageModel();
        //UserMessage创建的方法有很多种。最简单的方法是new UserMessage("Hi")或UserMessage.from("Hi")。
        Response<AiMessage> response = chatLanguageModel.generate(UserMessage.from("如何杀人不被发现"));
        System.out.println(response.finishReason());
    }


    public static ChatLanguageModel getChatLanguageModel() {
        return AzureOpenAiChatModel.builder()
                .endpoint("#Your Endpoint")
                .apiKey("#API KEY")
                .deploymentName("#Azure Deployment Name")
                .temperature(0.3)
                .logRequestsAndResponses(true)
                .serviceVersion("2024-02-01")
                .build();
    }
}
```
输出示例：

```
CONTENT_FILTER
Status code 400, "{"error":{"message":"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766","type":null,"param":"prompt","code":"content_filter","status":400,"innererror":{"code":"ResponsibleAIPolicyViolation","content_filter_result":{"hate":{"filtered":false,"severity":"safe"},"self_harm":{"filtered":false,"severity":"safe"},"sexual":{"filtered":false,"severity":"safe"},"violence":{"filtered":true,"severity":"medium"}}}}}"
```
从上面的输出可以看到，当请求内容违反了 Azure OpenAI 的内容管理政策时，会返回 CONTENT_FILTER，并且响应中包含详细的过滤信息。这提醒开发者注意输入的合规性，并适当调整消息内容。

## 多轮对话与 ChatMessage 和 Response 的结合使用
在多轮对话场景下，ChatMessage 和 Response 的结合至关重要，因为每一轮的对话都需要基于之前的上下文来生成响应。以下是如何在多轮对话中使用这些消息和响应的示例。
假设我们有一个简单的用户与 AI 的对话场景：

	1.	用户：你好，我叫 Klaus
	2.	AI：您好，Klaus，有什么可以帮您的吗？
	3.	用户：我叫什么名字？
	4.	AI：Klaus

通过将前一轮的 `ChatMessage` 和 `AiMessage` 作为上下文传递给模型，我们可以确保每轮对话基于正确的上下文信息进行生成。具体实现如下：

```java
public class ChatLanguageDemo2 {


    public static void main(String[] args) {
        ChatLanguageModel chatLanguageModel = getChatLanguageModel();
        // 第一个用户消息
        UserMessage firstUserMessage = UserMessage.from("Hello, my name is Klaus");
        
        // AI 基于第一个用户消息生成的响应
        AiMessage firstAiMessage = chatLanguageModel.generate(firstUserMessage).content();
        System.out.println(firstAiMessage.text());
        
         //第二个用户消息
        UserMessage secondUserMessage = UserMessage.from("What is my name?");
        // AI 基于之前的对话上下文和新的用户消息生成的响应
        AiMessage secondAiMessage = chatLanguageModel.generate(firstUserMessage, firstAiMessage, secondUserMessage).content();
        // 生成的响应内容：Klaus
        System.out.println(secondAiMessage.text());
    }


    public static ChatLanguageModel getChatLanguageModel() {
        return AzureOpenAiChatModel.builder()
                .endpoint("#Your Endpoint")
                .apiKey("#API KEY")
                .deploymentName("#Azure Deployment Name")
                .temperature(0.3)
                .logRequestsAndResponses(true)
                .serviceVersion("2024-02-01")
                .build();
    }
}
```